# -*- coding: utf-8 -*-
"""Post-Test-DTM-V3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1eVLNJOeCbj0Bq9vr_9Nold8L3NtEta0o

# **JUDUL ANALISIS**

Analisis Pertumbuhan Startup yang dilihat dari Tren Pendanaan, Valuasi, dan Profitabilitas di Berbagai industri

### **Studi Kasus**

Pertumbuhan startup dalam berbagai industri menunjukkan dinamika yang sangat cepat dalam beberapa tahun terakhir. Namun, tidak semua startup berhasil bertahan dan berkembang. Beberapa menghadapi tantangan dalam memperoleh pendanaan, mempertahankan valuasi yang sehat, dan mencapai profitabilitas. Dengan menganalisis data tren pendanaan, valuasi, dan profitabilitas, studi ini bertujuan untuk memahami faktor-faktor yang memengaruhi keberhasilan dan pertumbuhan startup di berbagai industri.

# **Import Library**
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer
from sklearn.cluster import KMeans, AgglomerativeClustering
from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score
from sklearn.model_selection import cross_val_score, KFold, GridSearchCV
from sklearn.feature_selection import SelectKBest, f_classif
from sklearn.decomposition import PCA
import warnings
from sklearn.exceptions import ConvergenceWarning
from scipy.cluster.hierarchy import dendrogram, linkage
import pickle
import joblib
import plotly.express as px
import plotly.graph_objects as go


warnings.filterwarnings('ignore', category=ConvergenceWarning)
warnings.filterwarnings('ignore', category=FutureWarning)
plt.style.use('ggplot')

"""# **Business Understanding**

### **Business Objective**

Tujuan utama dari analisis ini adalah untuk memahami tren pertumbuhan startup berdasarkan pendanaan, valuasi, dan profitabilitas di berbagai industri. Hal ini akan membantu pemangku kepentingan, seperti investor, pendiri startup, dan analis pasar, dalam mengambil keputusan strategis untuk mendukung keberlanjutan dan ekspansi bisnis.

### **Assess Situation**

Pasar startup sangat kompetitif dan cepat berubah. Setiap tahun, banyak startup yang menerima pendanaan awal, namun hanya sebagian kecil yang mampu berkembang hingga tahap profitabilitas. Dengan memahami tren pendanaan dan valuasi, serta hubungan keduanya dengan profitabilitas, analisis ini dapat memberikan wawasan penting terhadap dinamika pasar startup.

### **Data Mining Goals**

Tujuan utama dari analisis data ini adalah mengidentifikasi pola-pola dalam tren pendanaan dan valuasi startup yang berkorelasi dengan profitabilitas. Analisis ini mencakup faktor-faktor seperti jumlah pendanaan yang diterima, valuasi perusahaan, industri tempat startup beroperasi, serta status profitabilitasnya. Model prediktif akan digunakan untuk mengetahui faktor-faktor mana yang paling signifikan dalam mendorong pertumbuhan dan keberlanjutan startup

### **Project Plan**

Masukkan *Project Plan* kalian di sini (ganti aja tulisannya)

Perancangan proyek melibatkan serangkaian tahapan, termasuk pengumpulan dan pemrosesan data dari dataset Kaggle, eksplorasi data, analisis statistik, serta pembangunan model prediktif. Tujuannya adalah menghasilkan wawasan yang relevan dan berguna bagi pengambilan keputusan bisnis, serta memberikan pemahaman menyeluruh mengenai karakteristik pertumbuhan startup yang sukses.

### **Collect Initial Data**

Jelaskan darimana sumber data kalian di sini, sertakan linknya juga ya (ganti aja tulisan ini)
"""

df = pd.read_csv('startup_data.csv')
df.head()

"""### **Describe Data**

##### **Informasi Dasar**
"""

print("Dimensi dataset:")
print(f"Jumlah baris: {df.shape[0]}")
print(f"Jumlah kolom: {df.shape[1]}")

"""Dalam dataset ini ada 500 baris (startup) dan 12 kolom (atribut)

##### **Informasi Lanjutan**
"""

df.info()

"""Dataset ini berisi berbagai informasi terkait startup. Startup Name adalah nama perusahaan startup (tipe data: string), sedangkan Industry menunjukkan industri tempat startup beroperasi (tipe data: string). Funding Rounds mencatat jumlah putaran pendanaan yang telah diterima startup (tipe data: integer), dan Funding Amount (M USD) menunjukkan total pendanaan yang diterima dalam satuan juta USD (tipe data: float). Valuation (M USD) menggambarkan valuasi perusahaan dalam juta USD (tipe data: float), sementara Revenue (M USD) merepresentasikan pendapatan perusahaan dalam juta USD (tipe data: float). Employees mencatat jumlah karyawan yang dimiliki startup (tipe data: integer), dan Market Share (%) menunjukkan persentase pangsa pasar yang dikuasai (tipe data: float). Profitable adalah status profitabilitas startup, di mana 0 berarti belum profit dan 1 berarti sudah profit (tipe data: integer). Year Founded menunjukkan tahun didirikannya startup (tipe data: integer), sedangkan Region menunjukkan wilayah geografis tempat startup beroperasi (tipe data: string). Terakhir, Exit Status mendeskripsikan status exit startup, seperti Private, IPO, atau Acquired (tipe data: string).

##### **Statistika Deskriptif**
"""

df.describe().round(2)

"""Berdasarkan analisis deskriptif, Funding Rounds menunjukkan bahwa rata-rata startup telah melalui sekitar 3 hingga 4 putaran pendanaan, dengan nilai median sebesar 3 putaran. Terdapat startup yang belum menerima pendanaan sama sekali (minimum 0) dan ada yang telah melalui hingga 9 putaran. Untuk Funding Amount, rata-rata pendanaan yang diterima startup adalah sekitar 154 juta USD, dengan standar deviasi sebesar 173 juta USD, menunjukkan adanya variasi besar dalam jumlah pendanaan. Nilai median sebesar 78 juta USD mengindikasikan distribusi yang tidak simetris, di mana beberapa startup menerima pendanaan dalam jumlah sangat besar.

Pada aspek Valuation, rata-rata valuasi startup tercatat sekitar 1,8 miliar USD, dengan standar deviasi sebesar 2,2 miliar USD. Ada startup yang belum memiliki valuasi (nilai minimum 0), sementara yang terbesar mencapai hingga 10 miliar USD. Untuk Revenue, rata-rata pendapatan startup adalah sekitar 50 juta USD, dengan standar deviasi 53 juta USD, dan nilai median yang lebih rendah, yakni 32 juta USD, menunjukkan distribusi yang miring ke kanan.

Dalam hal jumlah karyawan (Employees), rata-rata startup mempekerjakan sekitar 778 orang, dengan variasi yang sangat besar (standar deviasi 752), dan jumlah karyawan berkisar dari 11 hingga 3000. Pada Market Share, rata-rata startup menguasai sekitar 3,6% pasar, dengan pangsa pasar maksimum mencapai 15,6%.

Untuk status profitabilitas (Profitable), sekitar 38% dari startup dalam dataset sudah mencapai kondisi profit (mean = 0,38). Sementara itu, dari sisi Year Founded, startup dalam dataset didirikan dalam rentang tahun 1995 hingga 2020, dengan rata-rata tahun pendirian sekitar tahun 2011.

### **Exploratory Data Analysis**

##### **Distribution**
"""

plt.figure(figsize=(12, 6))
industry_counts = df['Industry'].value_counts()
sns.barplot(x=industry_counts.index, y=industry_counts.values)
plt.title('Jumlah Startup per Industri')
plt.xticks(rotation=45, ha='right')
plt.tight_layout()
plt.show()

print(f"Distribusi startup berdasarkan industri:\n{industry_counts}")

"""Dalam dataset, terdapat beberapa industri yang mendominasi, seperti EdthTech, FinTech , dan E-commerce, yang memiliki jumlah startup terbanyak. Secara umum, industri-industri berbasis teknologi menunjukkan representasi yang sangat kuat di dalam dataset."""

plt.figure(figsize=(8, 5))
profit_counts = df['Profitable'].value_counts()
plt.pie(profit_counts, labels=['Belum Profit', 'Sudah Profit'], autopct='%1.1f%%', startangle=90, colors=['#ff9999','#66b3ff'])
plt.title('Distribusi Startup Berdasarkan Status Profitabilitas')
plt.axis('equal')
plt.show()

"""Mayoritas startup dalam dataset, yaitu sekitar 56.8%, belum mencapai profitabilitas. Hanya sekitar 43.2% startup yang sudah berhasil mencatatkan keuntungan, menunjukkan bahwa terdapat tantangan signifikan yang dihadapi startup dalam perjalanan menuju profitabilitas.

##### **Composition**
"""

plt.figure(figsize=(10, 6))
region_counts = df['Region'].value_counts()
sns.barplot(x=region_counts.index, y=region_counts.values)
plt.title('Distribusi Startup Berdasarkan Region')
plt.xticks(rotation=45, ha='right')
plt.tight_layout()
plt.show()

"""Sebagian besar startup dalam dataset berasal dari Australia, diikuti oleh Asia. Distribusi yang tidak merata ini mencerminkan adanya konsentrasi ekosistem startup yang kuat di beberapa wilayah tertentu, terutama di kawasan-kawasan dengan infrastruktur pendukung dan akses pendanaan yang lebih baik.

##### **Relationship**
"""

plt.figure(figsize=(10, 6))
sns.scatterplot(x='Funding Amount (M USD)', y='Valuation (M USD)', hue='Profitable', data=df, alpha=0.7)
plt.title('Hubungan antara Jumlah Pendanaan dan Valuasi')
plt.tight_layout()
plt.show()

"""Terdapat korelasi positif yang jelas antara jumlah pendanaan dan valuasi startup, di mana semakin besar pendanaan yang diterima, semakin tinggi pula valuasinya. Selain itu, startup yang sudah mencapai profitabilitas cenderung memiliki valuasi lebih tinggi dibandingkan dengan yang belum profit, bahkan pada tingkat pendanaan yang sama."""

plt.figure(figsize=(12, 8))
numeric_columns = ['Funding Rounds', 'Funding Amount (M USD)', 'Valuation (M USD)',
                   'Revenue (M USD)', 'Employees', 'Market Share (%)', 'Profitable', 'Year Founded']
correlation_matrix = df[numeric_columns].corr()
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt='.2f', linewidths=0.5)
plt.title('Matriks Korelasi Variabel Numerik')
plt.tight_layout()
plt.show()

"""Analisis korelasi menunjukkan adanya hubungan yang kuat antara berbagai variabel utama dalam dataset. Terdapat korelasi positif yang sangat kuat antara Funding Amount dan Valuation (sekitar 0,80), menunjukkan bahwa semakin besar pendanaan yang diterima, semakin tinggi valuasi startup tersebut. Selain itu, Revenue dan Employees juga menunjukkan korelasi positif baik dengan Valuation maupun Funding Amount, mengindikasikan bahwa perusahaan dengan pendanaan besar dan banyak karyawan cenderung memiliki valuasi lebih tinggi. Market Share berkorelasi positif dengan Revenue, yang menunjukkan bahwa startup dengan pangsa pasar lebih besar biasanya menghasilkan pendapatan yang lebih tinggi. Terakhir, Profitable juga berkorelasi positif dengan Revenue dan Market Share, mengindikasikan bahwa startup dengan pendapatan tinggi dan penguasaan pasar yang kuat lebih mungkin untuk mencapai profitabilitas.

##### **Comparison**
"""

plt.figure(figsize=(10, 6))
sns.boxplot(x='Profitable', y='Valuation (M USD)', data=df)
plt.title('Perbandingan Valuasi Berdasarkan Status Profitabilitas')
plt.xlabel('Status Profitabilitas (0=Belum Profit, 1=Sudah Profit)')
plt.ylabel('Valuasi (M USD)')
plt.tight_layout()
plt.show()

"""Startup yang telah mencapai profitabilitas umumnya memiliki median valuasi yang lebih tinggi dibandingkan dengan startup yang belum profit. Namun, terdapat pula sejumlah startup yang belum profit tetapi memiliki valuasi yang sangat tinggi, mencerminkan fenomena "unicorn" — perusahaan dengan valuasi besar meskipun belum menghasilkan keuntungan."""

plt.figure(figsize=(10, 6))
sns.boxplot(x='Profitable', y='Employees', data=df)
plt.title('Perbandingan Jumlah Karyawan Berdasarkan Status Profitabilitas')
plt.xlabel('Status Profitabilitas (0=Belum Profit, 1=Sudah Profit)')
plt.ylabel('Jumlah Karyawan')
plt.tight_layout()
plt.show()

"""Startup yang telah mencapai profitabilitas cenderung memiliki jumlah karyawan yang lebih banyak dibandingkan dengan yang belum profit. Hal ini menunjukkan adanya korelasi antara skala operasi, yang diukur dari jumlah karyawan, dengan kemampuan startup untuk mencapai profitabilitas."""

plt.figure(figsize=(14, 7))
sns.boxplot(x='Industry', y='Funding Amount (M USD)', data=df)
plt.title('Perbandingan Jumlah Pendanaan Berdasarkan Industri')
plt.xticks(rotation=45, ha='right')
plt.tight_layout()
plt.show()

"""Jumlah pendanaan yang diterima startup bervariasi secara signifikan antar industri. Beberapa industri besar seperti Gaming dan E-commerce cenderung menarik pendanaan dalam jumlah yang lebih besar, mencerminkan minat investor yang tinggi di sektor-sektor tersebut. Sebaliknya, industri niche seperti Iot dan HealthTech biasanya menerima pendanaan yang relatif lebih kecil, menunjukkan adanya perbedaan prioritas pendanaan berdasarkan potensi pasar dan tingkat pertumbuhan industri.

### **Verify Data Quality**

##### **Missing Values**
"""

print("Jumlah nilai kosong per kolom:")
print(df.isnull().sum())

"""Berdasarkan hasil pengecekan, tidak ditemukan nilai kosong (missing values) pada dataset. Semua kolom memiliki data lengkap, yang merupakan keuntungan besar untuk proses analisis selanjutnya karena tidak perlu melakukan penanganan khusus untuk nilai kosong.

##### **Outliers Values**
"""

def check_outliers(df, columns):
    outliers_info = {}

    for col in columns:
        Q1 = df[col].quantile(0.25)
        Q3 = df[col].quantile(0.75)
        IQR = Q3 - Q1

        lower_bound = Q1 - 1.5 * IQR
        upper_bound = Q3 + 1.5 * IQR

        outliers = df[(df[col] < lower_bound) | (df[col] > upper_bound)]
        outliers_info[col] = {
            'count': len(outliers),
            'percentage': (len(outliers) / len(df)) * 100,
            'lower_bound': lower_bound,
            'upper_bound': upper_bound
        }

    return outliers_info

numeric_cols = ['Funding Rounds', 'Funding Amount (M USD)', 'Valuation (M USD)',
                'Revenue (M USD)', 'Employees', 'Market Share (%)', 'Year Founded']

outliers_result = check_outliers(df, numeric_cols)

for col, info in outliers_result.items():
    print(f"{col}: {info['count']} outliers ({info['percentage']:.2f}%)")
    print(f"  Lower bound: {info['lower_bound']:.2f}, Upper bound: {info['upper_bound']:.2f}")

# Visualisasi outliers dengan boxplot
plt.figure(figsize=(15, 10))
for i, col in enumerate(numeric_cols, 1):
    plt.subplot(3, 3, i)
    sns.boxplot(y=df[col])
    plt.title(f'Boxplot - {col}')
    plt.tight_layout()
plt.show()

"""Dalam dataset, terdapat sejumlah outliers pada berbagai variabel yang menunjukkan distribusi yang tidak merata dalam dunia startup. Funding Amount (M USD) memiliki sekitar 7,20% outliers, yang menunjukkan beberapa startup menerima pendanaan jauh di atas rata-rata. Valuation (M USD) juga mencatat sekitar 7,40% outliers, yang mengindikasikan adanya "unicorn" atau startup dengan valuasi sangat tinggi. Begitu pula dengan Revenue (M USD), yang memiliki 7,80% outliers, menunjukkan adanya beberapa startup dengan pendapatan yang jauh lebih tinggi dibandingkan mayoritas. Employees mencatatkan sekitar 6,40% outliers, dengan beberapa startup memiliki jumlah karyawan yang sangat besar, sedangkan Market Share (%) juga mencatatkan sekitar 7,40% outliers, dengan beberapa startup memiliki pangsa pasar yang jauh lebih tinggi.

Outliers ini tidak selalu mencerminkan kesalahan data, melainkan dapat menunjukkan karakteristik nyata dari dunia startup, di mana distribusi kesuksesan sangat tidak merata. Beberapa startup tumbuh menjadi sangat besar, sementara mayoritas tetap berada pada ukuran yang lebih moderat. Dalam analisis clustering, outliers ini perlu diperhatikan karena dapat mempengaruhi hasil clustering, namun tidak harus dihapus karena mewakili fenomena nyata yang terjadi di dunia startup.

##### **Duplicated Values**
"""

duplicate_rows = df.duplicated().sum()
print(f"Jumlah baris duplikat: {duplicate_rows}")

"""Tidak ditemukan baris duplikat dalam dataset. Ini adalah hal positif karena duplicated data dapat menyebabkan bias dalam analisis dan mempengaruhi hasil clustering.

##### **Inconsistent Values/Noise**
"""

print("Nilai unik pada kolom Industry:")
print(df['Industry'].unique())
print("\nNilai unik pada kolom Region:")
print(df['Region'].unique())
print("\nNilai unik pada kolom Exit Status:")
print(df['Exit Status'].unique())

"""Kolom Industry menunjukkan nilai-nilai yang konsisten dan sesuai dengan kategori industri yang umum dalam ekosistem startup, mencerminkan sektor-sektor yang relevan dan terstruktur dengan baik. Begitu pula dengan kolom Region, yang juga memiliki nilai yang konsisten dan mencerminkan wilayah geografis utama tempat startup beroperasi. Pada kolom Exit Status, nilai-nilai yang tercatat terdiri dari kategori yang umum, yaitu "Private", "IPO", dan "Acquired", yang merupakan status exit yang biasa dijumpai pada startup.

Secara keseluruhan, tidak terdeteksi adanya nilai yang inkonsisten atau noise yang signifikan dalam dataset ini. Data terlihat bersih dan siap untuk dianalisis lebih lanjut.

# **Data Preparation**

### **Data Cleaning**

##### **Handling Outliers Value**
"""

def handle_outliers_with_capping(df, columns):
    df_processed = df.copy()

    for col in columns:
        Q1 = df[col].quantile(0.25)
        Q3 = df[col].quantile(0.75)
        IQR = Q3 - Q1

        lower_bound = Q1 - 1.5 * IQR
        upper_bound = Q3 + 1.5 * IQR

        df_processed[col] = df_processed[col].clip(lower=lower_bound, upper=upper_bound)

    return df_processed


outlier_handling_cols = ['Funding Amount (M USD)', 'Valuation (M USD)',
                        'Revenue (M USD)', 'Employees', 'Market Share (%)']

df_processed = handle_outliers_with_capping(df, outlier_handling_cols)

for col in outlier_handling_cols:
    plt.figure(figsize=(12, 5))

    plt.subplot(1, 2, 1)
    sns.boxplot(y=df[col])
    plt.title(f'Before - {col}')

    plt.subplot(1, 2, 2)
    sns.boxplot(y=df_processed[col])
    plt.title(f'After - {col}')

    plt.tight_layout()
    plt.show()

"""Setelah melakukan penanganan outliers dengan metode capping, nilai-nilai ekstrem pada kolom Funding Amount, Valuation, Revenue, Employees, dan Market Share telah dibatasi pada batas atas dan bawah yang lebih masuk akal, berdasarkan distribusi data. Hal ini membantu menyeimbangkan distribusi data dan mengontrol rentang nilai yang sangat lebar, yang dapat mempengaruhi kinerja algoritma clustering.

Metode capping dipilih daripada menghapus outliers karena beberapa alasan penting. Pertama, metode ini mempertahankan jumlah observasi dalam dataset, sehingga tidak ada kehilangan informasi penting. Kedua, capping tetap mencerminkan karakteristik startup yang sukses, namun dengan nilai yang lebih proporsional dan tidak ekstrem. Ketiga, dengan membatasi nilai ekstrem, metode ini mengurangi distorsi yang dapat terjadi dalam proses clustering, sehingga memungkinkan algoritma untuk bekerja lebih efektif dalam mengelompokkan data yang relevan.

### **Construct Data**

Minimal tambahkan satu fitur/kolom yang relevan dari dataset kalian ya

##### **Feature Engineering**
"""

# 1. Menambahkan fitur Age (usia startup)
current_year = 2023  # Asumsikan tahun saat ini
df_processed['Age'] = current_year - df_processed['Year Founded']

# 2. Menambahkan fitur Funding Efficiency
df_processed['Funding Efficiency'] = df_processed['Valuation (M USD)'] / df_processed['Funding Amount (M USD)']
# Mengatasi pembagian dengan nol atau nilai yang sangat kecil
df_processed['Funding Efficiency'] = df_processed['Funding Efficiency'].replace([np.inf, -np.inf], np.nan).fillna(0)

# 3. Menambahkan fitur Revenue per Employee
df_processed['Revenue per Employee'] = df_processed['Revenue (M USD)'] / df_processed['Employees']
df_processed['Revenue per Employee'] = df_processed['Revenue per Employee'].replace([np.inf, -np.inf], np.nan).fillna(0)

# 4. Menambahkan fitur kategori berdasarkan usia
def categorize_age(age):
    if age <= 3:
        return 'Early Stage'
    elif age <= 7:
        return 'Growth Stage'
    else:
        return 'Mature Stage'

df_processed['Stage'] = df_processed['Age'].apply(categorize_age)

"""**Age (Usia Startup)** dihitung sebagai selisih antara tahun saat ini (2023) dan tahun pendirian startup. Fitur ini penting karena usia startup dapat mempengaruhi tahap perkembangan, kemungkinan profitabilitas, dan pola pendanaan. Startup yang lebih tua cenderung lebih stabil dan mungkin telah melalui lebih banyak putaran pendanaan.

**Funding Efficiency** dihitung sebagai rasio antara valuasi dan jumlah pendanaan yang diterima. Metrik ini mengukur seberapa efisien startup dalam mengonversi investasi menjadi valuasi. Nilai yang lebih tinggi menunjukkan startup yang mampu mencapai valuasi tinggi dengan pendanaan yang relatif rendah, mencerminkan penggunaan dana yang lebih efisien.

**Revenue per Employee**  dihitung sebagai pendapatan dibagi jumlah karyawan, yang mengukur efisiensi operasional dan produktivitas startup. Startup dengan nilai tinggi pada metrik ini umumnya memiliki model bisnis yang lebih skalabel dan efisien, dengan potensi untuk tumbuh lebih cepat.

Terakhir, **Stage (Tahap Perkembangan)** mengkategorikan startup berdasarkan usia mereka menjadi tiga kategori: 'Early Stage', 'Growth Stage', dan 'Mature Stage'. Fitur kategorikal ini memberikan wawasan tentang tahap perkembangan startup dan membantu dalam interpretasi hasil clustering, memungkinkan pengelompokan berdasarkan tahapan yang relevan dalam siklus hidup startup.

Fitur-fitur ini menambah dimensi baru dalam analisis, mengungkapkan aspek-aspek penting seperti efisiensi, produktivitas, dan tahap perkembangan yang tidak secara langsung tersedia dalam dataset asli.

##### **Pengecekkan Hasil**
"""

print("Dataset setelah feature engineering:")
df_processed.head()

"""### **Data Reduction**

Minimal ada kolom yang kalian kurangin atau hapus
"""

df_reduced = df_processed.drop(['Startup Name', 'Year Founded'], axis=1)

"""Kolom **'Startup Name'** dihapus karena hanya berfungsi sebagai identifier unik dan tidak memberikan informasi yang dapat digunakan untuk analisis pola. Mempertahankan kolom ini justru dapat menyebabkan noise dalam proses clustering, karena tidak berhubungan dengan karakteristik yang ingin dianalisis.

Kolom **'Year Founded'** juga dihapus karena informasi ini telah dikonversi menjadi fitur yang lebih bermakna, yaitu 'Age' dan 'Stage'. Tahun pendirian startup sendiri kurang bermakna dalam konteks clustering dibandingkan dengan usia startup yang lebih menggambarkan tahap perkembangan. Dengan menghapus kolom ini, redundansi dalam dataset dapat dihindari.

Pengurangan dimensi ini membantu menyaring informasi yang lebih relevan, sehingga model clustering menjadi lebih sederhana, lebih efisien, dan hasilnya lebih mudah diinterpretasikan.

##### **Pengecekkan Hasil**
"""

print("Kolom setelah data reduction:")
print(df_reduced.columns.tolist())

"""### **Data Transformation**

##### **Mapping**
"""

exit_status_mapping = {
    'Private': 0,
    'IPO': 1,
    'Acquired': 2
}
df_reduced['Exit Status Numeric'] = df_reduced['Exit Status'].map(exit_status_mapping)

"""kolom 'Exit Status' yang bersifat kategorikal diubah menjadi nilai numerik menggunakan pengkodean ordinal (0, 1, 2) untuk mewakili status "Private", "IPO", dan "Acquired". Hal ini diperlukan karena algoritma clustering seperti K-Means bekerja lebih baik dengan data numerik, dan pengkodean ordinal ini juga mempertahankan hierarki logis dari status exit, di mana "Private" < "IPO" < "Acquired".

##### **Encoding**

Encoding wajib dilakukan
"""

categorical_cols = ['Industry', 'Region', 'Stage']
df_encoded = pd.get_dummies(df_reduced, columns=categorical_cols, drop_first=False)

df_encoded = df_encoded.drop(['Exit Status'], axis=1)

print("Dataset setelah mapping dan encoding:")
print(df_encoded.head())
print(f"\nJumlah fitur setelah encoding: {df_encoded.shape[1]}")

"""One-Hot Encoding diterapkan pada kolom-kolom kategorikal seperti 'Industry', 'Region', dan 'Stage'. Setiap nilai unik dalam kolom-kolom tersebut diubah menjadi kolom dummy terpisah dengan nilai 0 atau 1. Proses ini memungkinkan algoritma clustering untuk memproses fitur kategorikal tanpa menganggap adanya hubungan ordinal. Sebagai contoh, industri seperti 'FinTech' dan 'HealthTech' akan dikonversi menjadi kolom terpisah, yang memungkinkan model untuk mengidentifikasi pola-pola yang lebih jelas.

Terakhir, kolom 'Exit Status' yang asli dihapus setelah dikonversi menjadi 'Exit Status Numeric' untuk menghindari redundansi dalam dataset.

### **Saving Data**
"""

df_encoded.to_csv('startup_data_processed.csv', index=False)

"""#**Modelling**

###**Select Modelling Techniques**

#####**Algoritma 1 (K-Means)**

K-Means adalah algoritma clustering yang membagi data menjadi k kelompok berdasarkan jarak ke centroid, dengan tujuan meminimalkan jarak kuadrat antara data dan centroid. Kelebihannya termasuk kesederhanaan, efisiensi pada dataset besar, dan performa baik pada cluster berbentuk bola dan ukuran seragam. Namun, K-Means membutuhkan penentuan jumlah cluster (k) terlebih dahulu, sensitif terhadap pemilihan centroid awal, tidak efektif untuk cluster non-spherical, dan sangat dipengaruhi oleh adanya outliers.

#####**Algoritma 2 (Agglomerative)**

Agglomerative Clustering adalah metode hierarchical clustering yang bersifat bottom-up, dimulai dengan setiap data sebagai cluster individu, kemudian menggabungkan pasangan cluster terdekat secara berulang hingga semua tergabung. Kelebihannya adalah tidak memerlukan penentuan jumlah cluster terlebih dahulu, dapat mengidentifikasi cluster dengan bentuk tidak beraturan, memberikan struktur hierarki yang memungkinkan analisis pada berbagai level, dan lebih stabil terhadap outliers dengan metode linkage tertentu. Namun, kelemahannya meliputi kompleksitas komputasi yang lebih tinggi (O(n²log(n))) dibandingkan K-Means (O(nkt)), kesulitan pada dataset besar, dan hasil yang dapat bergantung pada metode pengukuran jarak serta linkage yang dipilih.

###**Pre-Processing**
"""

numeric_features = ['Funding Rounds', 'Funding Amount (M USD)', 'Valuation (M USD)',
                    'Revenue (M USD)', 'Employees', 'Market Share (%)', 'Profitable',
                    'Age', 'Funding Efficiency', 'Revenue per Employee', 'Exit Status Numeric']


scaler = StandardScaler()
df_scaled = df_encoded.copy()
df_scaled[numeric_features] = scaler.fit_transform(df_scaled[numeric_features])

print("Statistik data setelah standardisasi:")
df_scaled[numeric_features].describe().round(2)

"""Standardisasi data dilakukan untuk memastikan semua fitur memiliki skala yang sama, dengan mean 0 dan standard deviation 1, yang penting karena beberapa alasan. Pertama, algoritma clustering seperti K-Means dan Agglomerative Clustering sangat sensitif terhadap skala data, dan fitur dengan range nilai besar dapat mendominasi perhitungan jarak. Sebagai contoh, fitur seperti 'Valuation (M USD)' yang memiliki nilai jauh lebih besar daripada 'Funding Rounds' bisa mengubah hasil clustering tanpa standardisasi. Standardisasi memastikan bahwa semua fitur berkontribusi secara proporsional dalam perhitungan jarak. Fitur hasil one-hot encoding tidak perlu di-standardisasi karena sudah memiliki skala yang sama (0 atau 1). Hasil standardisasi menunjukkan bahwa semua fitur numerik sekarang memiliki mean mendekati 0 dan standard deviation mendekati 1, yang optimal untuk algoritma clustering.

###**Build and Train Model**

#####**Algoritma 1**
"""

inertia = []
silhouette_scores = []
k_range = range(2, 11)

for k in k_range:
    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)
    kmeans.fit(df_scaled)
    inertia.append(kmeans.inertia_)

    # Hitung silhouette score
    if k > 1:  # Silhouette score membutuhkan minimal 2 cluster
        silhouette = silhouette_score(df_scaled, kmeans.labels_)
        silhouette_scores.append(silhouette)
    else:
        silhouette_scores.append(0)

# Plot elbow method
plt.figure(figsize=(12, 5))

plt.subplot(1, 2, 1)
plt.plot(k_range, inertia, 'bo-')
plt.xlabel('Jumlah Cluster (k)')
plt.ylabel('Inertia')
plt.title('Elbow Method untuk K-Means')
plt.grid(True)

plt.subplot(1, 2, 2)
plt.plot(k_range, silhouette_scores, 'ro-')
plt.xlabel('Jumlah Cluster (k)')
plt.ylabel('Silhouette Score')
plt.title('Silhouette Score untuk K-Means')
plt.grid(True)

plt.tight_layout()
plt.show()

"""Berikan penjelasan di sini ya"""

optimal_k = 4  # Berdasarkan analisis elbow method dan silhouette score

# Training model K-Means dengan jumlah cluster optimal
kmeans_model = KMeans(n_clusters=optimal_k, random_state=42, n_init=10)
kmeans_clusters = kmeans_model.fit_predict(df_scaled)

# Menambahkan hasil clustering ke dataframe asli
df_reduced['KMeans_Cluster'] = kmeans_clusters

# Visualisasi hasil clustering dengan PCA untuk reduksi dimensi
pca = PCA(n_components=2)
df_pca = pca.fit_transform(df_scaled)

# Plot hasil clustering
plt.figure(figsize=(10, 8))
scatter = plt.scatter(df_pca[:, 0], df_pca[:, 1], c=kmeans_clusters, cmap='viridis', alpha=0.8)
plt.title('K-Means Clustering (Visualisasi dengan PCA)')
plt.xlabel('Principal Component 1')
plt.ylabel('Principal Component 2')
plt.colorbar(scatter, label='Cluster')
plt.grid(True, alpha=0.3)
plt.tight_layout()
plt.show()

# Analisis karakteristik cluster
cluster_analysis = df_reduced.groupby('KMeans_Cluster').agg({
    'Funding Rounds': 'mean',
    'Funding Amount (M USD)': 'mean',
    'Valuation (M USD)': 'mean',
    'Revenue (M USD)': 'mean',
    'Employees': 'mean',
    'Market Share (%)': 'mean',
    'Profitable': 'mean',
    'Age': 'mean',
    'Funding Efficiency': 'mean',
    'Revenue per Employee': 'mean',
    'Exit Status Numeric': 'mean'
}).round(2)

print("Karakteristik cluster K-Means:")
print(cluster_analysis)

"""#####**Algoritma 2**"""

plt.figure(figsize=(12, 8))
linked = linkage(df_scaled, method='ward')
dendrogram(linked, orientation='top', distance_sort='descending', show_leaf_counts=True)
plt.title('Dendrogram - Agglomerative Clustering')
plt.xlabel('Sample Index')
plt.ylabel('Distance')
plt.axhline(y=6, color='r', linestyle='--')  # Threshold untuk potensial jumlah cluster
plt.show()

# Menentukan jumlah optimal cluster berdasarkan dendrogram
optimal_n_clusters = 4  # Berdasarkan analisis dendrogram

# Training model Agglomerative Clustering
agglomerative_model = AgglomerativeClustering(n_clusters=optimal_n_clusters, linkage='ward')
agglomerative_clusters = agglomerative_model.fit_predict(df_scaled)

# Menambahkan hasil clustering ke dataframe
df_reduced['Agglomerative_Cluster'] = agglomerative_clusters

# Visualisasi hasil clustering dengan PCA
plt.figure(figsize=(10, 8))
scatter = plt.scatter(df_pca[:, 0], df_pca[:, 1], c=agglomerative_clusters, cmap='plasma', alpha=0.8)
plt.title('Agglomerative Clustering (Visualisasi dengan PCA)')
plt.xlabel('Principal Component 1')
plt.ylabel('Principal Component 2')
plt.colorbar(scatter, label='Cluster')
plt.grid(True, alpha=0.3)
plt.tight_layout()
plt.show()

"""Berikan penjelasan di sini ya"""

# Analisis karakteristik cluster
cluster_analysis_agg = df_reduced.groupby('Agglomerative_Cluster').agg({
    'Funding Rounds': 'mean',
    'Funding Amount (M USD)': 'mean',
    'Valuation (M USD)': 'mean',
    'Revenue (M USD)': 'mean',
    'Employees': 'mean',
    'Market Share (%)': 'mean',
    'Profitable': 'mean',
    'Age': 'mean',
    'Funding Efficiency': 'mean',
    'Revenue per Employee': 'mean',
    'Exit Status Numeric': 'mean'
}).round(2)

print("Karakteristik cluster Agglomerative:")
print(cluster_analysis_agg)

"""###**Begins and Compare Predict**"""

comparison = pd.crosstab(df_reduced['KMeans_Cluster'], df_reduced['Agglomerative_Cluster'],
                         rownames=['K-Means'], colnames=['Agglomerative'])
print("Perbandingan hasil clustering:")
print(comparison)

# Menghitung konsistensi antara kedua metode clustering
total_consistent = np.sum(np.diag(comparison))
consistency_percentage = (total_consistent / len(df_reduced)) * 100

print(f"\nKonsistensi antara K-Means dan Agglomerative: {consistency_percentage:.2f}%")

fig, axes = plt.subplots(1, 2, figsize=(16, 7))

# Plot untuk K-Means
scatter1 = axes[0].scatter(df_pca[:, 0], df_pca[:, 1], c=kmeans_clusters, cmap='viridis', alpha=0.8)
axes[0].set_title('K-Means Clustering')
axes[0].set_xlabel('Principal Component 1')
axes[0].set_ylabel('Principal Component 2')
fig.colorbar(scatter1, ax=axes[0], label='Cluster')
axes[0].grid(True, alpha=0.3)

# Plot untuk Agglomerative
scatter2 = axes[1].scatter(df_pca[:, 0], df_pca[:, 1], c=agglomerative_clusters, cmap='plasma', alpha=0.8)
axes[1].set_title('Agglomerative Clustering')
axes[1].set_xlabel('Principal Component 1')
axes[1].set_ylabel('Principal Component 2')
fig.colorbar(scatter2, ax=axes[1], label='Cluster')
axes[1].grid(True, alpha=0.3)

plt.tight_layout()
plt.show()

"""Berikan penjelasannya di sini ya

# Evaluation
"""

def evaluate_clustering(data, kmeans_labels, agg_labels):
    metrics = {}

    # Silhouette Score - rentang -1 hingga 1, lebih tinggi = lebih baik
    metrics['K-Means Silhouette'] = silhouette_score(data, kmeans_labels)
    metrics['Agglomerative Silhouette'] = silhouette_score(data, agg_labels)

    # Calinski-Harabasz Index - lebih tinggi = lebih baik
    metrics['K-Means Calinski-Harabasz'] = calinski_harabasz_score(data, kmeans_labels)
    metrics['Agglomerative Calinski-Harabasz'] = calinski_harabasz_score(data, agg_labels)

    # Davies-Bouldin Index - lebih rendah = lebih baik
    metrics['K-Means Davies-Bouldin'] = davies_bouldin_score(data, kmeans_labels)
    metrics['Agglomerative Davies-Bouldin'] = davies_bouldin_score(data, agg_labels)

    return metrics

# Evaluasi model clustering
evaluation_metrics = evaluate_clustering(df_scaled, kmeans_clusters, agglomerative_clusters)

# Tampilkan hasil evaluasi
print("Hasil Evaluasi Model Clustering:")
for metric, value in evaluation_metrics.items():
    print(f"{metric}: {value:.4f}")

# Visualisasi metrik evaluasi
plt.figure(figsize=(12, 5))

# Metrik yang lebih tinggi = lebih baik
metrics_higher_better = ['Silhouette', 'Calinski-Harabasz']
# Metrik yang lebih rendah = lebih baik
metrics_lower_better = ['Davies-Bouldin']

# Plot metrik yang lebih tinggi = lebih baik
plt.subplot(1, 2, 1)
x = ['Silhouette', 'Calinski-Harabasz']
y_kmeans = [evaluation_metrics['K-Means Silhouette'],
           evaluation_metrics['K-Means Calinski-Harabasz']/1000]  # Scaling untuk visualisasi
y_agg = [evaluation_metrics['Agglomerative Silhouette'],
         evaluation_metrics['Agglomerative Calinski-Harabasz']/1000]  # Scaling untuk visualisasi

x_pos = np.arange(len(x))
width = 0.35

plt.bar(x_pos - width/2, y_kmeans, width, label='K-Means')
plt.bar(x_pos + width/2, y_agg, width, label='Agglomerative')
plt.ylabel('Nilai (Lebih Tinggi = Lebih Baik)')
plt.title('Metrik Evaluasi (Lebih Tinggi = Lebih Baik)')
plt.xticks(x_pos, x)
plt.legend()

# Plot metrik yang lebih rendah = lebih baik
plt.subplot(1, 2, 2)
x = ['Davies-Bouldin']
y_kmeans = [evaluation_metrics['K-Means Davies-Bouldin']]
y_agg = [evaluation_metrics['Agglomerative Davies-Bouldin']]

x_pos = np.arange(len(x))

plt.bar(x_pos - width/2, y_kmeans, width, label='K-Means')
plt.bar(x_pos + width/2, y_agg, width, label='Agglomerative')
plt.ylabel('Nilai (Lebih Rendah = Lebih Baik)')
plt.title('Metrik Evaluasi (Lebih Rendah = Lebih Baik)')
plt.xticks(x_pos, x)
plt.legend()

plt.tight_layout()
plt.show()

def kmeans_stability(data, k, n_trials=5):
    scores = []
    for i in range(n_trials):
        # Split data menggunakan K-Fold
        kf = KFold(n_splits=5, shuffle=True, random_state=i)
        fold_scores = []

        for train_idx, test_idx in kf.split(data):
            train_data = data.iloc[train_idx]
            test_data = data.iloc[test_idx]

            # Fit model pada training data
            km = KMeans(n_clusters=k, random_state=i, n_init=10)
            km.fit(train_data)

            # Predict pada test data
            labels = km.predict(test_data)

            # Hitung silhouette score jika memungkinkan (perlu minimal 2 label)
            if len(np.unique(labels)) > 1:
                score = silhouette_score(test_data, labels)
                fold_scores.append(score)

        scores.append(np.mean(fold_scores))

    return np.mean(scores), np.std(scores)

def agg_stability(data, k, n_trials=5):
    scores = []
    for i in range(n_trials):
        # Split data menggunakan K-Fold
        kf = KFold(n_splits=5, shuffle=True, random_state=i)
        fold_scores = []

        for train_idx, test_idx in kf.split(data):
            train_data = data.iloc[train_idx]

            # Fit model pada training data
            agg = AgglomerativeClustering(n_clusters=k, linkage='ward')
            train_labels = agg.fit_predict(train_data)

            # Perlu melatih model untuk test data juga, karena Agglomerative tidak memiliki metode .predict()
            test_data = data.iloc[test_idx]
            test_labels = AgglomerativeClustering(n_clusters=k, linkage='ward').fit_predict(test_data)

            # Hitung silhouette score jika memungkinkan
            if len(np.unique(test_labels)) > 1:
                score = silhouette_score(test_data, test_labels)
                fold_scores.append(score)

        scores.append(np.mean(fold_scores))

    return np.mean(scores), np.std(scores)

k_stability_mean, k_stability_std = kmeans_stability(df_scaled, optimal_k)
agg_stability_mean, agg_stability_std = agg_stability(df_scaled, optimal_n_clusters)

print(f"Stabilitas K-Means (Cross-Validation): Mean Silhouette = {k_stability_mean:.4f}, Std = {k_stability_std:.4f}")
print(f"Stabilitas Agglomerative (Cross-Validation): Mean Silhouette = {agg_stability_mean:.4f}, Std = {agg_stability_std:.4f}")

# Visualisasi hasil cross-validation
plt.figure(figsize=(8, 5))
plt.bar(['K-Means', 'Agglomerative'], [k_stability_mean, agg_stability_mean], yerr=[k_stability_std, agg_stability_std], alpha=0.7)
plt.ylabel('Mean Silhouette Score (Cross-Validation)')
plt.title('Stabilitas Algoritma Clustering')
plt.grid(axis='y', alpha=0.3)
plt.show()

y_clusters = kmeans_clusters

# Menerapkan SelectKBest untuk menemukan fitur terpenting
selector = SelectKBest(f_classif, k=10)
X_new = selector.fit_transform(df_scaled, y_clusters)

# Mendapatkan indeks fitur yang dipilih
selected_indices = selector.get_support(indices=True)
selected_features = df_scaled.columns[selected_indices]

# Mendapatkan skor untuk setiap fitur
feature_scores = pd.DataFrame({
    'Feature': df_scaled.columns,
    'Score': selector.scores_
})
feature_scores = feature_scores.sort_values('Score', ascending=False)

print("Top 10 fitur terpenting untuk clustering:")
print(feature_scores.head(10))

# Visualisasi feature importance
plt.figure(figsize=(12, 6))
sns.barplot(x='Score', y='Feature', data=feature_scores.head(10))
plt.title('Top 10 Fitur Terpenting untuk Clustering')
plt.tight_layout()
plt.show()

param_grid = {
    'n_clusters': [3, 4, 5, 6],
    'init': ['k-means++', 'random'],
    'n_init': [10, 20],
    'max_iter': [200, 300],
    'algorithm': ['lloyd', 'elkan']
}

# Siapkan model K-Means
kmeans_tuning = KMeans(random_state=42)

# Grid Search dengan silhouette score sebagai metrik
grid_search = GridSearchCV(
    kmeans_tuning,
    param_grid,
    cv=5,
    scoring=lambda estimator, X: silhouette_score(X, estimator.fit_predict(X)),
    verbose=0
)

# Fit Grid Search
grid_search.fit(df_scaled)

# Tampilkan hasil terbaik
print("Best parameters:", grid_search.best_params_)
print("Best silhouette score:", grid_search.best_score_)

# Buat model dengan parameter terbaik
kmeans_best = KMeans(**grid_search.best_params_, random_state=42)
kmeans_best_clusters = kmeans_best.fit_predict(df_scaled)

kmeans_original_silhouette = silhouette_score(df_scaled, kmeans_clusters)
kmeans_tuned_silhouette = silhouette_score(df_scaled, kmeans_best_clusters)

print(f"Silhouette Score - Original K-Means: {kmeans_original_silhouette:.4f}")
print(f"Silhouette Score - Tuned K-Means: {kmeans_tuned_silhouette:.4f}")
print(f"Improvement: {(kmeans_tuned_silhouette - kmeans_original_silhouette) / kmeans_original_silhouette * 100:.2f}%")

# Visualisasi perbandingan sebelum dan sesudah tuning
plt.figure(figsize=(10, 5))
scores = [kmeans_original_silhouette, kmeans_tuned_silhouette]
plt.bar(['Original K-Means', 'Tuned K-Means'], scores, color=['blue', 'green'])
plt.ylabel('Silhouette Score')
plt.title('Perbandingan Kinerja Sebelum dan Sesudah Hyperparameter Tuning')
plt.grid(axis='y', alpha=0.3)

# Tambahkan nilai di atas bar
for i, v in enumerate(scores):
    plt.text(i, v + 0.01, f"{v:.4f}", ha='center')

plt.tight_layout()
plt.show()

with open('kmeans_best_model.pkl', 'wb') as file:
    pickle.dump(kmeans_best, file)

with open('standard_scaler.pkl', 'wb') as file:
    pickle.dump(scaler, file)

print("Model dan scaler berhasil disimpan.")

with open('kmeans_best_model.pkl', 'rb') as file:
    loaded_model = pickle.load(file)

sample_prediction = loaded_model.predict(df_scaled[:5])
print(f"Contoh prediksi dari model yang dimuat: {sample_prediction}")

joblib.dump(kmeans_best, 'kmeans_best_model.joblib')

joblib.dump(scaler, 'standard_scaler.joblib')

print("Model dan scaler berhasil disimpan dengan joblib.")

loaded_model = joblib.load('kmeans_best_model.joblib')
sample_prediction = loaded_model.predict(df_scaled[:5])
print(f"Contoh prediksi dari model yang dimuat dengan joblib: {sample_prediction}")